Developers have 2 way of building code, using Plang service or OpenAI. This is a guide on how to use each

## plang 
You can use plang service to build your code or if you have OpenAI api key you can use that.

When you use plang service, you are supporting the project. 
The plang service cost excatly 2x what using Open AI key would cost.
To use plang service, you just start building. 

On first build, if you dont have any voucher on the plang service, you will be provided
with a payment link. Click that, choose the amount you want to buy for, something small to start with, like $5
fill in the creditcard information, and submit. 

The build again with plang, and you should be on your way.

## openai
To use OpenAI, you can run with the 
 you first need a api key(https://openai.com/). 

then you can use the `--llmservice=openai` parameter, e.g. 

`plang --llmservice=openai`
`plang build --llmservice=openai`
`plang exec --llmservice=openai`

## environment variable
You can  add to your environment variable, `PLangLllmService`, acceptable values are `plang`, `openai`. If you do this, you don't need to use --llmservice parameter when running plang

you might need to restart any tool that will use the variable

## Visual code
You can set Visual code to use either plang or openai as the default LLM service, in the Settings, search for `Select Plang LLM service`

## LocalLLM

There is no local llm as of yet, but if you want to create one and try it you can make plang to it. Add environment variable `PLangLllmServiceUrl`, the value must start with 'http'. If you have your local llm running on port 5000, your value should be http://localhost:5000/path/to/llm/

You will recieve the standard OpenAI json request(https://platform.openai.com/docs/guides/text-generation/json-mode) from the plang language


# Which is better
Plang service uses gpt4o from OpenAI, so there is no difference in results. 
Hopefully in the future we can provide you with faster and much cheaper service
By using our service your are supporting the project, development 
and hopefully enabling us to create a cheaper llm 
as the build process is relativly simple and doesn't need such a large LLM